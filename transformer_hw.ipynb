{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7607,
     "status": "ok",
     "timestamp": 1610084063632,
     "user": {
      "displayName": "Sheikh Jubair",
      "photoUrl": "",
      "userId": "05924530152827167322"
     },
     "user_tz": 360
    },
    "id": "Y3Agee_VPKHv",
    "outputId": "185c0cd6-dec8-49dc-b37d-0733b7f42b38"
   },
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install pandas\n",
    "# !pip3 install pickle5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7817,
     "status": "ok",
     "timestamp": 1610084063860,
     "user": {
      "displayName": "Sheikh Jubair",
      "photoUrl": "",
      "userId": "05924530152827167322"
     },
     "user_tz": 360
    },
    "id": "FXKQ7cFMPTpG",
    "outputId": "45351d88-a3e1-4dfc-efae-3e3f2f9ba899"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# %cd /content/drive/MyDrive/Barley/single_env_barley/\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Q7T8WyAVLbpA"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "5zQNapifLbpF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iwb2m0UDm8Ku"
   },
   "outputs": [],
   "source": [
    "ATTENTION_HEAD = 2 \n",
    "EMBID_DIM= 6\n",
    "NLAYERS = 2\n",
    "FEEDFORWARD_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "4wZHkT8mLbpG"
   },
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "torch.manual_seed(100)\n",
    "NUM_FEATURES = 24866\n",
    "KFOLD = 5\n",
    "MUTUAL_INFO_THRESH = 0.05\n",
    "FOLD = 4\n",
    "MAX_EPOCH = 1000\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kycHkoj-LbpG"
   },
   "outputs": [],
   "source": [
    "training_file = '../Dataset/tr_hw_fold_{}.pkl'\n",
    "test_file = '../Dataset/test_hw_fold_{}.pkl'\n",
    "feature_file = '../Dataset/hw_selected_features.pkl'\n",
    "validation_file = '../Dataset/validation_hw.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EU4yi7JULbpH"
   },
   "outputs": [],
   "source": [
    "class BarleyDataset(Dataset):\n",
    "    def __init__(self, file_path, feature_file):\n",
    "        with open(file_path, 'rb') as pfile:\n",
    "            self.data = pickle.load(pfile)\n",
    "\n",
    "        with open(feature_file, 'rb') as pfile:\n",
    "            self.selected_features = pickle.load(pfile)\n",
    "           \n",
    "        self.labels = self.data.iloc[:, -2:].to_numpy()\n",
    "        self.data = self.data.iloc[:,:-2].to_numpy()\n",
    "        self.transform()\n",
    "        \n",
    "\n",
    "\n",
    "    def transform(self):\n",
    "        d = []\n",
    "        for row in self.data:\n",
    "            row = np.array(row)\n",
    "            d.append(row)\n",
    "        self.data = np.array(d)\n",
    "\n",
    "        self.data = self.data[:,self.selected_features]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        return self.data[ind], self.labels[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "tFvNSTb0LbpH"
   },
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, attention_head, num_features, embed_dim, output_dim = 2, feedforward_dim = 256, dropout = 0.1, nlayers= 2):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        self.embedding = nn.Linear(num_features, num_features * embed_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(embed_dim, attention_head, feedforward_dim, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        # self.fc_random = nn.Linear(num_markers * num_features, 768)\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.fc = nn.Linear(num_features * embed_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = x.view(x.shape[0], -1)\n",
    "        # x = self.fc_random(x)\n",
    "        x = self.embedding(x).view(x.shape[0], -1, self.embed_dim)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "eYLS7I3bD0Hh"
   },
   "outputs": [],
   "source": [
    "training_file = training_file.format(FOLD)\n",
    "tr_barleyDataset = BarleyDataset(training_file, feature_file) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8774,
     "status": "ok",
     "timestamp": 1610084064867,
     "user": {
      "displayName": "Sheikh Jubair",
      "photoUrl": "",
      "userId": "05924530152827167322"
     },
     "user_tz": 360
    },
    "id": "sn7QkDiaywIC",
    "outputId": "f1dc1593-1353-4172-8b49-8fd9f6164064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "712\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(tr_barleyDataset.data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "ebxE_qn5MJE_"
   },
   "outputs": [],
   "source": [
    "tr_loader = DataLoader(tr_barleyDataset, batch_size = 8, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8760,
     "status": "ok",
     "timestamp": 1610084064868,
     "user": {
      "displayName": "Sheikh Jubair",
      "photoUrl": "",
      "userId": "05924530152827167322"
     },
     "user_tz": 360
    },
    "id": "_CAHGwnCThiJ",
    "outputId": "7f4d91f5-68bc-4138-f97e-341aa359d59c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80, 9833)\n"
     ]
    }
   ],
   "source": [
    "val_dataset = BarleyDataset(validation_file, feature_file)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)\n",
    "print(val_dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9002,
     "status": "ok",
     "timestamp": 1610084065119,
     "user": {
      "displayName": "Sheikh Jubair",
      "photoUrl": "",
      "userId": "05924530152827167322"
     },
     "user_tz": 360
    },
    "id": "S3J_e0AhzqbM",
    "outputId": "5a409a79-0ea5-4b5d-abd3-6b82e6fefe60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681\n"
     ]
    }
   ],
   "source": [
    "print(len(np.unique(val_dataset.data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MbhaSPpPnziz"
   },
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ycV5E6j-LbpK"
   },
   "outputs": [],
   "source": [
    "# x = torch.rand(16, 1000, 1)\n",
    "# print(x)\n",
    "# model = TransformerModel(1, 1000, 1)\n",
    "\n",
    "model = TransformerModel(attention_head = ATTENTION_HEAD, num_features = tr_barleyDataset.data.shape[1], \n",
    "                         embed_dim= EMBID_DIM, nlayers = NLAYERS, feedforward_dim= FEEDFORWARD_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZCuodrwZLbpL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 204463,
     "status": "ok",
     "timestamp": 1610084260602,
     "user": {
      "displayName": "Sheikh Jubair",
      "photoUrl": "",
      "userId": "05924530152827167322"
     },
     "user_tz": 360
    },
    "id": "AGVwsLWpLbpM",
    "outputId": "c268034e-b3f0-4e80-eaa1-6238528e3a98"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.16 GiB (GPU 0; 7.92 GiB total capacity; 6.49 GiB already allocated; 433.62 MiB free; 6.49 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-bdf69a1bb38a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mtr_running_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bioinfo/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/bioinfo/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     89\u001b[0m                         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                         \u001b[0;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                         \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg_sq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'amsgrad'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                             \u001b[0;31m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.16 GiB (GPU 0; 7.92 GiB total capacity; 6.49 GiB already allocated; 433.62 MiB free; 6.49 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "    \n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "best_avg_pcc = -1\n",
    "best_fhb_pcc = 0\n",
    "best_don_pcc = 0\n",
    "\n",
    "best_model = model.state_dict()\n",
    "\n",
    "no_improve = 0\n",
    "best_epoch = 0\n",
    "for epoch in range(MAX_EPOCH):\n",
    "    tr_running_loss = 0\n",
    "    count = 0\n",
    "    pcc_fhb = 0\n",
    "    pcc_don = 0\n",
    "    model.train()\n",
    "    for genos, phenos in tr_loader:\n",
    "        #print(genos)\n",
    "        genos = genos.to(device)\n",
    "        genos = genos.float()\n",
    "        \n",
    "        phenos = phenos.to(device)\n",
    "        phenos = phenos.float()\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(genos)\n",
    "\n",
    "       \n",
    "        #boxes = torch.FloatTe(boxes)\n",
    "        loss = criterion(outputs, phenos)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        tr_running_loss += loss.item()\n",
    "        pcc_fhb += pearsonr(phenos[:, 0].detach().cpu().numpy(), outputs[:, 0].detach().cpu().numpy())[0]\n",
    "        pcc_don += pearsonr(phenos[:, 1].detach().cpu().numpy(), outputs[:, 1].detach().cpu().numpy() )[0]\n",
    "        \n",
    "        count+=1\n",
    "    \n",
    "    tr_avg_pcc = ((pcc_fhb + pcc_don) / 2) / count\n",
    "\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    print('TRAIN LOSS: {}, AVG PCC: {}, FHB PCC: {}, DON PCC: {}'.format(\n",
    "        tr_running_loss / len(tr_loader), tr_avg_pcc, pcc_fhb / count, \n",
    "        pcc_don / count))\n",
    "    print('********')\n",
    "\n",
    "\n",
    "    count = 0\n",
    "    pcc_fhb = 0\n",
    "    pcc_don = 0\n",
    "    validation_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for genos, phenos in val_loader:\n",
    "            genos = genos.to(device)\n",
    "            genos = genos.float()\n",
    "            \n",
    "            phenos = phenos.to(device)\n",
    "            phenos = phenos.float()\n",
    "\n",
    "            outputs = model(genos)\n",
    "            \n",
    "            loss = criterion(outputs, phenos)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            pcc_fhb += pearsonr(phenos[:, 0].detach().cpu().numpy(), outputs[:, 0].detach().cpu().numpy())[0]\n",
    "            pcc_don += pearsonr(phenos[:, 1].detach().cpu().numpy(), outputs[:, 1].detach().cpu().numpy() )[0]\n",
    "            count+=1\n",
    "\n",
    "\n",
    "    val_avg_pcc = ((pcc_fhb + pcc_don) / 2) / count\n",
    "    \n",
    "    print('VAL LOSS: {}, AVG PCC: {} FHB PCC: {}, DON PCC: {}'.format( \n",
    "          validation_loss / len(val_loader), val_avg_pcc, pcc_fhb / count, \n",
    "          pcc_don / count))\n",
    "    \n",
    "    if best_avg_pcc < val_avg_pcc:\n",
    "        best_avg_pcc = val_avg_pcc\n",
    "        best_fhb_pcc = pcc_fhb / count\n",
    "        best_don_pcc = pcc_don / count\n",
    "        best_model = model.state_dict()\n",
    "        no_improve = 0\n",
    "        best_epoch = epoch\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    print('********')\n",
    "    print('BEST EPOCH: {} AVG PCC: {}, FHB PCC: {}, DON PCC: {}'.format(\n",
    "        best_epoch, best_avg_pcc, best_fhb_pcc, best_don_pcc))\n",
    "    \n",
    "    print('===============================================================================')\n",
    "    print()\n",
    "    print()\n",
    "    if no_improve >= 20:\n",
    "        break\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DB33XWaNlgIb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpMzmd0SLbpM"
   },
   "outputs": [],
   "source": [
    "torch.save(best_model, '../outputs/model_' + str(FOLD) + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FjLng_Mnj5y"
   },
   "source": [
    "### TEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221265,
     "status": "ok",
     "timestamp": 1610084277424,
     "user": {
      "displayName": "Sheikh Jubair",
      "photoUrl": "",
      "userId": "05924530152827167322"
     },
     "user_tz": 360
    },
    "id": "8W8Bo0aRms3W",
    "outputId": "5313ae50-28cb-4a59-edae-9c3536e18bc7"
   },
   "outputs": [],
   "source": [
    "test_dataset = BarleyDataset(test_file.format(FOLD), feature_file)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=True)\n",
    "print(test_dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUApvHX2n-yP"
   },
   "outputs": [],
   "source": [
    "model = TransformerModel(attention_head = ATTENTION_HEAD, num_features = tr_barleyDataset.data.shape[1], \n",
    "                         embed_dim= EMBID_DIM, nlayers = NLAYERS, feedforward_dim= FEEDFORWARD_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5Gr_EbNsiQB"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('../outputs/model_' + str(FOLD) + '.pt', map_location=device))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 228142,
     "status": "ok",
     "timestamp": 1610084284327,
     "user": {
      "displayName": "Sheikh Jubair",
      "photoUrl": "",
      "userId": "05924530152827167322"
     },
     "user_tz": 360
    },
    "id": "nRV1ua8goz4u",
    "outputId": "f3383ab8-253c-4823-ec20-4a6e0b80de53"
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "pcc_fhb = 0\n",
    "pcc_don = 0\n",
    "test_loss = 0\n",
    "\n",
    "model.eval()\n",
    "true_fhb = []\n",
    "true_don = []\n",
    "predicted_fhb = []\n",
    "predicted_don = []\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for genos, phenos in test_loader:\n",
    "        genos = genos.to(device)\n",
    "        genos = genos.float()\n",
    "        \n",
    "        phenos = phenos.to(device)\n",
    "        phenos = phenos.float()\n",
    "\n",
    "        outputs = model(genos)\n",
    "        \n",
    "        loss = criterion(outputs, phenos)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        pcc_fhb += pearsonr(phenos[:, 0].detach().cpu().numpy(), outputs[:, 0].detach().cpu().numpy())[0]\n",
    "        pcc_don += pearsonr(phenos[:, 1].detach().cpu().numpy(), outputs[:, 1].detach().cpu().numpy() )[0]\n",
    "\n",
    "        predicted_fhb += outputs[:, 0].detach().cpu().numpy().tolist()\n",
    "        predicted_don += outputs[:, 1].detach().cpu().numpy().tolist()\n",
    "\n",
    "        true_fhb += phenos[:, 0].detach().cpu().numpy().tolist()\n",
    "        true_don += phenos[:, 1].detach().cpu().numpy().tolist()\n",
    "        count+=1\n",
    "\n",
    "    test_avg_pcc = ((pcc_fhb + pcc_don) / 2) / count\n",
    "    \n",
    "    print('TEST LOSS: {}, AVG PCC: {} FHB PCC: {}, DON PCC: {}'.format( \n",
    "          test_loss / len(test_loader), test_avg_pcc, pcc_fhb / count, \n",
    "          pcc_don / count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G7ovaUNOrct7"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'True FHB': true_fhb,\n",
    "                   'Predicted FHB': predicted_fhb,\n",
    "                   'True DON': true_don,\n",
    "                   'Predicted DON': predicted_don})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 228616,
     "status": "ok",
     "timestamp": 1610084284817,
     "user": {
      "displayName": "Sheikh Jubair",
      "photoUrl": "",
      "userId": "05924530152827167322"
     },
     "user_tz": 360
    },
    "id": "y1H6Wuj8qlhj",
    "outputId": "b7ce4c13-91ce-4ac7-c360-feeae23439fc"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "\n",
    "fig = px.scatter(df, x=\"True FHB\", y=\"Predicted FHB\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 228865,
     "status": "ok",
     "timestamp": 1610084285079,
     "user": {
      "displayName": "Sheikh Jubair",
      "photoUrl": "",
      "userId": "05924530152827167322"
     },
     "user_tz": 360
    },
    "id": "toqVK_7KrvAs",
    "outputId": "43d4db26-4762-4995-f2f2-994489fb32dd"
   },
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x=\"True DON\", y=\"Predicted DON\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J6XTh4UgTlVV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 228849,
     "status": "ok",
     "timestamp": 1610084285080,
     "user": {
      "displayName": "Sheikh Jubair",
      "photoUrl": "",
      "userId": "05924530152827167322"
     },
     "user_tz": 360
    },
    "id": "oFJDkM-HTulP",
    "outputId": "b1eb0ee3-ed5f-4244-e6da-b4a0cea3ce33"
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.scatter(x= df['True FHB'].to_numpy(), y=df['Predicted FHB'].to_numpy())\n",
    "plt.xlabel('True FHB')\n",
    "plt.ylabel('Predicted FHB')\n",
    "plt.show()\n",
    "plt.savefig('../outputs/fhb_' + str(FOLD) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 229148,
     "status": "ok",
     "timestamp": 1610084285390,
     "user": {
      "displayName": "Sheikh Jubair",
      "photoUrl": "",
      "userId": "05924530152827167322"
     },
     "user_tz": 360
    },
    "id": "eWKPT6OTU9nb",
    "outputId": "015df5f6-f93b-4779-fdae-04d27938b69c"
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.scatter(x= df['True DON'].to_numpy(), y=df['Predicted DON'].to_numpy())\n",
    "plt.xlabel('True DON')\n",
    "plt.ylabel('Predicted DON')\n",
    "plt.show()\n",
    "plt.savefig('../outputs/don_' + str(FOLD) + '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ztH0JcVqWnId"
   },
   "outputs": [],
   "source": [
    "with open('../outputs/fhb_'+ str(FOLD) + '.pkl', 'wb') as file:\n",
    "    pickle.dump(df[['True FHB', 'Predicted FHB']], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liA-UJRaXiAG"
   },
   "outputs": [],
   "source": [
    "with open('../outputs/don_'+ str(FOLD) + '.pkl', 'wb') as file:\n",
    "    pickle.dump(df[['True DON', 'Predicted DON']], file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "transformer_hw.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_kernel",
   "language": "python",
   "name": "conda_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
